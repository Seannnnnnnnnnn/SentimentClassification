{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8c1a09",
   "metadata": {},
   "source": [
    "### Sentiment Analysis - GloVe100\n",
    "\n",
    "In this notebook, we work with the `GloVe100` embedding representations of tweets, in combination with other simple features that are engineered from the raw tweet string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3621ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import floor, sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bfc93",
   "metadata": {},
   "source": [
    "### Data Loading & Preprocessing\n",
    "\n",
    "We do the usual data loading and preprocessing, we apply a min-max scaling of the data to bring all feature values to a $[-1, 1]$ range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea0a584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df_csv) -> pd.DataFrame:\n",
    "    # loads df_glove data\n",
    "    df_glove = []\n",
    "\n",
    "    for i in range(df_csv.shape[0]):\n",
    "        glove = ast.literal_eval(df_csv['tweet'].iloc[i])\n",
    "        df_glove.append(glove)\n",
    "        \n",
    "    return pd.DataFrame(df_glove, columns=[\"f\"+str(i) for i in range(1, 101)]), df_csv['sentiment']\n",
    "\n",
    "df_from_csv = pd.read_csv(\"train_glove.csv\")\n",
    "X, y = load_data(df_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c0e18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # we create this as a global variable\n",
    "    \n",
    "\n",
    "def num_chars(tweet: str):\n",
    "    return len(tweet)\n",
    "\n",
    "\n",
    "def num_words(tweet: str):\n",
    "    return len(tweet.split())\n",
    "\n",
    "\n",
    "def num_cap_words(tweet: str):\n",
    "    return sum(map(str.isupper, tweet.split()))\n",
    "\n",
    "\n",
    "def num_hashtags(tweet: str): \n",
    "    return tweet.count('#')\n",
    "\n",
    "\n",
    "def num_mentions(tweet: str):\n",
    "    return tweet.count('@')\n",
    "\n",
    "\n",
    "def num_stopwords(tweet: str):  \n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    stopwords_tweet = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_tweet)\n",
    "\n",
    "def num_punctuations(tweet: str):\n",
    "    punctuations='!\"$%&\\'()*+,-./:;<=>?[\\]^_`{|}~'\n",
    "    count = 0\n",
    "    for char in tweet:\n",
    "        if char in punctuations: count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "engineered_features = [num_chars, num_words, num_cap_words, num_hashtags, num_mentions, \n",
    "                       num_stopwords, num_punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e4af288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_features(filename: str):\n",
    "    df_raw = pd.read_csv(filename)\n",
    "    \n",
    "    for feature in engineered_features:         \n",
    "        df_raw[feature.__name__] = df_raw[\"tweet\"].apply(lambda x: feature(x))\n",
    "    \n",
    "    df_raw['avg_wordlength'] = df_raw['num_chars']/df_raw['num_words']    \n",
    "    df_raw.drop(['sentiment', 'tweet_id', 'tweet'], axis=1, inplace=True)\n",
    "    return df_raw\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame):\n",
    "    # Preprocessing applied a min-max scaling of raw engineered features\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    scaled = min_max_scaler.fit_transform(df)\n",
    "    return pd.DataFrame(scaled, columns = [feature.__name__ for feature in engineered_features]+['avg_wordlength'])\n",
    "\n",
    "\n",
    "X_raw    = load_raw_features(\"train_full.csv\")\n",
    "X_raw_scaled = preprocess(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f640f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full data set by merging orginal X with raw features\n",
    "assert X_raw.shape[0] == X.shape[0]\n",
    "\n",
    "X_full        = pd.merge(X, X_raw, left_index=True, right_index=True)\n",
    "X_full_scaled = pd.merge(X, X_raw_scaled, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669529ad",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "We now fit the datasets to a variety of traditional statistical learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c8f30766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "l2_logistic_raw = LogisticRegression(penalty='l2', random_state=0, max_iter=10000).fit(X_full, y)\n",
    "l1_logistic_raw = LogisticRegression(penalty='none',random_state=0, max_iter=10000).fit(X_full, y)\n",
    "\n",
    "l2_logistic_std = LogisticRegression(penalty='l2', random_state=0, max_iter=10000).fit(X_full_scaled, y)\n",
    "l1_logistic_std = LogisticRegression(penalty='none',random_state=0, max_iter=10000).fit(X_full_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ffc95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gaussian_nb_raw = GaussianNB().fit(X_full, y)\n",
    "gaussian_nb_std = GaussianNB().fit(X_full_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7cb7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "L2_knn_raw = KNeighborsClassifier(n_neighbors=floor(sqrt(X.shape[0]))).fit(X_full, y)\n",
    "L2_knn_std = KNeighborsClassifier(n_neighbors=floor(sqrt(X.shape[0]))).fit(X_full_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce530cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_raw = RandomForestClassifier(max_depth=None, random_state=0).fit(X_full, y)\n",
    "RF_std = RandomForestClassifier(max_depth=None, random_state=0).fit(X_full_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488c372",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "We now evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3ce72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_pipeline(X: pd.DataFrame):\n",
    "    return load_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f45943dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19906 testing instances, 108 features\n"
     ]
    }
   ],
   "source": [
    "TrainingSet = pd.read_csv('dev_glove.csv')\n",
    "X_test, y_test = transformation_pipeline(TrainingSet)\n",
    "\n",
    "X_test_raw = load_raw_features('dev_full.csv')\n",
    "X_test_raw_scaled = preprocess(X_test_raw)\n",
    "\n",
    "\n",
    "X_test_full = pd.merge(X_test, X_test_raw, left_index=True, right_index=True)\n",
    "X_test_full_scaled = pd.merge(X_test, X_test_raw_scaled, left_index=True, right_index=True)\n",
    "print(f\"{X_test.shape[0]} testing instances, {X_test_full.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ddc8e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Logistic Regression Accuary score : 0.6962222445493821\n",
      "Raw Logistic Regression f1 score      : 0.6962222445493821\n",
      "Std Logistic Regression Accuary score : 0.6888877725308952\n",
      "Std Logistic Regression f1 score      : 0.6888877725308952\n",
      "\n",
      "Raw Gaussian Naive Bayes Accuary score   : 0.4594092233497438\n",
      "Raw Gaussian Naive Bayes f1 score        : 0.4594092233497438\n",
      "Std Gaussian Naive Bayes Accuary score   : 0.46046418165377273\n",
      "Std Gaussian Naive Bayes f1 score        : 0.4604641816537728\n",
      "\n",
      "Raw Gaussian Naive Bayes Accuary score   : 0.6218225660604842\n",
      "Raw Gaussian Naive Bayes f1 score        : 0.6218225660604842\n",
      "Std Gaussian Naive Bayes Accuary score   : 0.613935496835125\n",
      "Std Gaussian Naive Bayes f1 score        : 0.613935496835125\n",
      "\n",
      "Raw RF Accuary score   : 0.682507786597006\n",
      "Raw RF f1 score        : 0.682507786597006\n",
      "Std RF Accuary score   : 0.680548578318095\n",
      "Std RF f1 score        : 0.680548578318095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "l2_lr_pred     = l1_logistic_raw.predict(X_test_full)\n",
    "l2_lr_pred_std = l1_logistic_std.predict(X_test_full_scaled)\n",
    "\n",
    "print(f\"Raw Logistic Regression Accuary score : {accuracy_score(l2_lr_pred, y_test)}\")\n",
    "print(f\"Raw Logistic Regression f1 score      : {f1_score(l2_lr_pred, y_test, average='micro')}\")\n",
    "print(f\"Std Logistic Regression Accuary score : {accuracy_score(l2_lr_pred_std, y_test)}\")\n",
    "print(f\"Std Logistic Regression f1 score      : {f1_score(l2_lr_pred_std, y_test, average='micro')}\\n\")\n",
    "\n",
    "\n",
    "nb_pred     = gaussian_nb_raw.predict(X_test_full)\n",
    "nb_pred_std = gaussian_nb_std.predict(X_test_full_scaled)\n",
    "print(f\"Raw Gaussian Naive Bayes Accuary score   : {accuracy_score(nb_pred, y_test)}\")\n",
    "print(f\"Raw Gaussian Naive Bayes f1 score        : {f1_score(nb_pred, y_test, average='micro')}\")\n",
    "print(f\"Std Gaussian Naive Bayes Accuary score   : {accuracy_score(nb_pred_std, y_test)}\")\n",
    "print(f\"Std Gaussian Naive Bayes f1 score        : {f1_score(nb_pred_std, y_test, average='micro')}\\n\")\n",
    "\n",
    "\n",
    "knn_pred     = L2_knn_raw.predict(X_test_full)\n",
    "knn_pred_std = L2_knn_std.predict(X_test_full_scaled)\n",
    "print(f\"Raw Gaussian Naive Bayes Accuary score   : {accuracy_score(knn_pred, y_test)}\")\n",
    "print(f\"Raw Gaussian Naive Bayes f1 score        : {f1_score(knn_pred, y_test, average='micro')}\")\n",
    "print(f\"Std Gaussian Naive Bayes Accuary score   : {accuracy_score(knn_pred_std, y_test)}\")\n",
    "print(f\"Std Gaussian Naive Bayes f1 score        : {f1_score(knn_pred_std, y_test, average='micro')}\\n\")\n",
    "\n",
    "\n",
    "RF_raw_pred = RF_raw.predict(X_test_full)\n",
    "RF_std_pred = RF_std.predict(X_test_full_scaled)\n",
    "print(f\"Raw RF Accuary score   : {accuracy_score(RF_raw_pred, y_test)}\")\n",
    "print(f\"Raw RF f1 score        : {f1_score(RF_raw_pred, y_test, average='micro')}\")\n",
    "print(f\"Std RF Accuary score   : {accuracy_score(RF_std_pred, y_test)}\")\n",
    "print(f\"Std RF f1 score        : {f1_score(RF_std_pred, y_test, average='micro')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
